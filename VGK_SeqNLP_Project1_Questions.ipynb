{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaiml/prj11-nlp-Project-1/blob/master/VGK_SeqNLP_Project1_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# VGK- IMDB-Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "#vocab_size = 10000 #vocab size\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEsHYrWxdtk",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L3N_KwUWQL3",
        "colab_type": "text"
      },
      "source": [
        "1. Import test and train data (5 Marks)\n",
        "2. Import the labels ( train and test) (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j53MFHbRXORF",
        "colab_type": "code",
        "outputId": "16455211-1dc0-48e3-e3cf-e6e27fbd7b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print (x_train[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
            "  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26    4\n",
            "  715    8  118 1634   14  394   20   13  119  954  189  102    5  207\n",
            "  110 3103   21   14   69  188    8   30   23    7    4  249  126   93\n",
            "    4  114    9 2300 1523    5  647    4  116    9   35 8163    4  229\n",
            "    9  340 1322    4  118    9    4  130 4901   19    4 1002    5   89\n",
            "   29  952   46   37    4  455    9   45   43   38 1543 1905  398    4\n",
            " 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775    7\n",
            " 8255    2  349 2637  148  605    2 8003   15  123  125   68    2 6853\n",
            "   15  349  165 4362   98    5    4  228    9   43    2 1157   15  299\n",
            "  120    5  120  174   11  220  175  136   50    9 4373  228 8255    5\n",
            "    2  656  245 2350    5    4 9837  131  152  491   18    2   32 7464\n",
            " 1212   14    9    6  371   78   22  625   64 1382    9    8  168  145\n",
            "   23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n",
            "   89   78  285   16  145   95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noyCd8ReaXrf",
        "colab_type": "code",
        "outputId": "1603a50a-b16d-4e5b-80fa-c94bd027ca16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print (x_train[1].shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6n-uM2eCy2",
        "colab_type": "code",
        "outputId": "2c8dba3f-a9e8-41bf-8b65-903229f27f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_train[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZhMAgaNeCy5",
        "colab_type": "code",
        "outputId": "2020ebcd-b8ac-4368-cd5b-58e93eb9b199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     1]\n",
            " [12500 12500]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXIGrKayY3hX",
        "colab_type": "code",
        "outputId": "1dcbcce1-f75f-46a1-b758-a1c7290f4698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     1]\n",
            " [12500 12500]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-WNfQCWcR1",
        "colab_type": "text"
      },
      "source": [
        "## WORD INDEX BUILDING\n",
        "\n",
        "3. Get the word index and then Create a key-value pair for word and word_id (12.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNemWT1xgyWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDQMCIl6kH6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ref :: https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
        "reverse_word_map = dict(map(reversed, word_index.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8UN2M8clJjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function takes a tokenized sentence and returns the words\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WseygeMvlQl1",
        "colab_type": "code",
        "outputId": "2437a87d-c183-44d7-a8c7-511a6341958c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#test\n",
        "review = sequence_to_text(x_train[0])\n",
        "print(review)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "\n",
        "4. Build a Sequential Model using Keras for the Sentiment Classification task (10 points)\n",
        "\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zT45K7c0TbB",
        "colab_type": "code",
        "outputId": "4b7dd11a-62ae-47f7-ddfe-74fce5c1672f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM\n",
        "### create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, trainable=True, input_length=maxlen))\n",
        "model.add(LSTM(units=64, dropout=0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "### Fit the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=500, verbose=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 300, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,331,521\n",
            "Trainable params: 1,331,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 44s 2ms/step - loss: 0.5664 - acc: 0.7094 - val_loss: 0.3631 - val_acc: 0.8521\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.2737 - acc: 0.8920 - val_loss: 0.2961 - val_acc: 0.8732\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1895 - acc: 0.9288 - val_loss: 0.3134 - val_acc: 0.8757\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1545 - acc: 0.9444 - val_loss: 0.3373 - val_acc: 0.8712\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1158 - acc: 0.9596 - val_loss: 0.3987 - val_acc: 0.8668\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.0929 - acc: 0.9684 - val_loss: 0.3984 - val_acc: 0.8643\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.0926 - acc: 0.9680 - val_loss: 0.4583 - val_acc: 0.8588\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.0696 - acc: 0.9767 - val_loss: 0.6266 - val_acc: 0.8479\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1486 - acc: 0.9428 - val_loss: 0.4576 - val_acc: 0.8594\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 43s 2ms/step - loss: 0.0643 - acc: 0.9777 - val_loss: 0.5495 - val_acc: 0.8569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7a091c8f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6gdpYmWorf",
        "colab_type": "text"
      },
      "source": [
        "## Model Accuracy\n",
        "5. Report the Accuracy of the model (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56y3I_75WwKr",
        "colab_type": "code",
        "outputId": "03a8affd-46cd-496a-9ebc-10ed8963a7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cqs6jSjWwNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPMQFTc0M47u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f70dc651-2b39-47f9-fcf4-dc53f4c67a6d"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01475677]\n",
            " [0.9999519 ]\n",
            " [0.36325932]\n",
            " ...\n",
            " [0.00889832]\n",
            " [0.04148394]\n",
            " [0.89531636]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbT9r3TKOsDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.round(y_pred, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_9Bf16OP_z-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a12c643-1eb7-4ff5-8ecd-6e8d53879002"
      },
      "source": [
        "y_pred = y_pred.ravel()\n",
        "y_pred.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-6o7bhkR5A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = y_pred.astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ehXqOvmSv23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3be8f6b-399b-4726-c197-d03b43a4fca9"
      },
      "source": [
        "y_test.ravel()\n",
        "y_test"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__GeLlIuMaYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "23534353-05a0-42ad-9fd6-408eddfca278"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Sentiment_Positive', 'Sentiment_Negative']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Sentiment_Positive       0.85      0.87      0.86     12500\n",
            "Sentiment_Negative       0.86      0.85      0.86     12500\n",
            "\n",
            "          accuracy                           0.86     25000\n",
            "         macro avg       0.86      0.86      0.86     25000\n",
            "      weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRBpiReKVRzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04b4295f-bb72-452f-a79b-45a8e80cabf2"
      },
      "source": [
        "sequence_to_text(x_test[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " 'the',\n",
              " 'wonder',\n",
              " 'own',\n",
              " 'as',\n",
              " 'by',\n",
              " 'is',\n",
              " 'sequence',\n",
              " 'i',\n",
              " 'i',\n",
              " 'and',\n",
              " 'and',\n",
              " 'to',\n",
              " 'of',\n",
              " 'hollywood',\n",
              " 'br',\n",
              " 'of',\n",
              " 'down',\n",
              " 'shouting',\n",
              " 'getting',\n",
              " 'boring',\n",
              " 'of',\n",
              " 'ever',\n",
              " 'it',\n",
              " 'sadly',\n",
              " 'sadly',\n",
              " 'sadly',\n",
              " 'i',\n",
              " 'i',\n",
              " 'was',\n",
              " 'then',\n",
              " 'does',\n",
              " \"don't\",\n",
              " 'close',\n",
              " 'faint',\n",
              " 'after',\n",
              " 'one',\n",
              " 'carry',\n",
              " 'as',\n",
              " 'by',\n",
              " 'are',\n",
              " 'be',\n",
              " 'favourites',\n",
              " 'all',\n",
              " 'family',\n",
              " 'turn',\n",
              " 'in',\n",
              " 'does',\n",
              " 'as',\n",
              " 'three',\n",
              " 'part',\n",
              " 'in',\n",
              " 'another',\n",
              " 'some',\n",
              " 'to',\n",
              " 'be',\n",
              " 'probably',\n",
              " 'with',\n",
              " 'world',\n",
              " 'and',\n",
              " 'her',\n",
              " 'an',\n",
              " 'have',\n",
              " 'faint',\n",
              " 'beginning',\n",
              " 'own',\n",
              " 'as',\n",
              " 'is',\n",
              " 'sequence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWjN98FHVmH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fef60667-7303-4825-e31d-9d30c312b971"
      },
      "source": [
        "sequence_to_text(x_test[1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " 'the',\n",
              " 'as',\n",
              " 'you',\n",
              " \"world's\",\n",
              " 'is',\n",
              " 'quite',\n",
              " 'br',\n",
              " 'mankind',\n",
              " 'most',\n",
              " 'that',\n",
              " 'quest',\n",
              " 'are',\n",
              " 'chase',\n",
              " 'to',\n",
              " 'being',\n",
              " 'quickly',\n",
              " 'of',\n",
              " 'little',\n",
              " 'it',\n",
              " 'time',\n",
              " 'hell',\n",
              " 'to',\n",
              " 'plot',\n",
              " 'br',\n",
              " 'of',\n",
              " 'something',\n",
              " 'long',\n",
              " 'put',\n",
              " 'are',\n",
              " 'of',\n",
              " 'every',\n",
              " 'place',\n",
              " 'this',\n",
              " 'consequence',\n",
              " 'and',\n",
              " 'of',\n",
              " 'interplay',\n",
              " 'storytelling',\n",
              " 'being',\n",
              " 'nasty',\n",
              " 'not',\n",
              " 'of',\n",
              " 'you',\n",
              " 'warren',\n",
              " 'in',\n",
              " 'is',\n",
              " 'failed',\n",
              " 'club',\n",
              " 'i',\n",
              " 'i',\n",
              " 'of',\n",
              " 'films',\n",
              " 'pay',\n",
              " 'so',\n",
              " 'sequences',\n",
              " 'and',\n",
              " 'film',\n",
              " 'okay',\n",
              " 'uses',\n",
              " 'to',\n",
              " 'received',\n",
              " 'and',\n",
              " 'if',\n",
              " 'time',\n",
              " 'done',\n",
              " 'for',\n",
              " 'room',\n",
              " 'sugar',\n",
              " 'viewer',\n",
              " 'as',\n",
              " 'cartoon',\n",
              " 'of',\n",
              " 'gives',\n",
              " 'to',\n",
              " 'forgettable',\n",
              " 'br',\n",
              " 'be',\n",
              " 'because',\n",
              " 'many',\n",
              " 'these',\n",
              " 'of',\n",
              " 'reflection',\n",
              " 'sugar',\n",
              " 'contained',\n",
              " 'gives',\n",
              " 'it',\n",
              " 'wreck',\n",
              " 'scene',\n",
              " 'to',\n",
              " 'more',\n",
              " 'was',\n",
              " 'two',\n",
              " 'when',\n",
              " 'had',\n",
              " 'find',\n",
              " 'as',\n",
              " 'you',\n",
              " 'another',\n",
              " 'it',\n",
              " 'of',\n",
              " 'themselves',\n",
              " 'probably',\n",
              " 'who',\n",
              " 'interplay',\n",
              " 'storytelling',\n",
              " 'if',\n",
              " 'itself',\n",
              " 'by',\n",
              " 'br',\n",
              " 'about',\n",
              " \"1950's\",\n",
              " 'films',\n",
              " 'not',\n",
              " 'would',\n",
              " 'effects',\n",
              " 'that',\n",
              " 'her',\n",
              " 'box',\n",
              " 'to',\n",
              " 'miike',\n",
              " 'for',\n",
              " 'if',\n",
              " 'hero',\n",
              " 'close',\n",
              " 'seek',\n",
              " 'end',\n",
              " 'is',\n",
              " 'very',\n",
              " 'together',\n",
              " 'movie',\n",
              " 'of',\n",
              " 'wheel',\n",
              " 'got',\n",
              " 'say',\n",
              " 'kong',\n",
              " 'sugar',\n",
              " 'fred',\n",
              " 'close',\n",
              " 'bore',\n",
              " 'there',\n",
              " 'is',\n",
              " 'playing',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'and',\n",
              " 'pan',\n",
              " 'place',\n",
              " 'trilogy',\n",
              " 'of',\n",
              " 'lacks',\n",
              " 'br',\n",
              " 'of',\n",
              " 'their',\n",
              " 'time',\n",
              " 'much',\n",
              " 'this',\n",
              " 'men',\n",
              " 'as',\n",
              " 'on',\n",
              " 'it',\n",
              " 'is',\n",
              " 'telling',\n",
              " 'program',\n",
              " 'br',\n",
              " 'silliness',\n",
              " 'okay',\n",
              " 'and',\n",
              " 'to',\n",
              " 'frustration',\n",
              " 'at',\n",
              " 'corner',\n",
              " 'and',\n",
              " 'she',\n",
              " 'of',\n",
              " 'sequences',\n",
              " 'to',\n",
              " 'political',\n",
              " 'clearly',\n",
              " 'in',\n",
              " 'of',\n",
              " 'drugs',\n",
              " 'keep',\n",
              " 'guy',\n",
              " 'i',\n",
              " 'i',\n",
              " 'was',\n",
              " 'throwing',\n",
              " 'room',\n",
              " 'sugar',\n",
              " 'as',\n",
              " 'it',\n",
              " 'by',\n",
              " 'br',\n",
              " 'be',\n",
              " 'plot',\n",
              " 'many',\n",
              " 'for',\n",
              " 'occasionally',\n",
              " 'film',\n",
              " 'verge',\n",
              " 'boyfriend',\n",
              " 'difficult',\n",
              " 'kid',\n",
              " 'as',\n",
              " 'you',\n",
              " 'it',\n",
              " 'failed',\n",
              " 'not',\n",
              " 'if',\n",
              " 'gerard',\n",
              " 'to',\n",
              " 'if',\n",
              " 'woman',\n",
              " 'in',\n",
              " 'and',\n",
              " 'is',\n",
              " 'police',\n",
              " 'fi',\n",
              " 'spooky',\n",
              " 'or',\n",
              " 'of',\n",
              " 'self',\n",
              " 'what',\n",
              " 'have',\n",
              " 'pretty',\n",
              " 'in',\n",
              " 'can',\n",
              " 'so',\n",
              " 'suit',\n",
              " 'you',\n",
              " 'good',\n",
              " '2',\n",
              " 'which',\n",
              " 'why',\n",
              " 'super',\n",
              " 'as',\n",
              " 'it',\n",
              " 'main',\n",
              " 'of',\n",
              " 'my',\n",
              " 'i',\n",
              " 'i',\n",
              " '\\x96',\n",
              " 'if',\n",
              " 'time',\n",
              " 'screenplay',\n",
              " 'in',\n",
              " 'same',\n",
              " 'this',\n",
              " 'remember',\n",
              " 'assured',\n",
              " 'have',\n",
              " 'action',\n",
              " 'one',\n",
              " 'in',\n",
              " 'realistic',\n",
              " 'that',\n",
              " 'better',\n",
              " 'of',\n",
              " 'lessons']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built\n",
        "\n",
        "6. Retrieve the output of each layer in Keras for a given single test sample from the trained model you built (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AqOnLa2eCzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "349803e6-2fc3-4b4c-e81a-84d472fdf832"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "inp = model.input                                           # input placeholder\n",
        "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
        "print (outputs)\n",
        "functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
        "\n",
        "# Testing\n",
        "test = x_test[0][np.newaxis,...]\n",
        "layer_outs = [func([test, 1.]) for func in functors]\n",
        "\n",
        "\n",
        "print (layer_outs)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'embedding_2/embedding_lookup/Identity:0' shape=(?, 300, 128) dtype=float32>, <tf.Tensor 'lstm_2/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'dense_3/Relu:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'dense_4/Sigmoid:0' shape=(?, 1) dtype=float32>]\n",
            "[[array([[[ 0.09708548, -0.04185277, -0.0233365 , ..., -0.04557361,\n",
            "          0.04528384, -0.03951082],\n",
            "        [ 0.09708548, -0.04185277, -0.0233365 , ..., -0.04557361,\n",
            "          0.04528384, -0.03951082],\n",
            "        [ 0.09708548, -0.04185277, -0.0233365 , ..., -0.04557361,\n",
            "          0.04528384, -0.03951082],\n",
            "        ...,\n",
            "        [ 0.01653893,  0.00948247, -0.00811201, ..., -0.0069657 ,\n",
            "         -0.00347264, -0.04094478],\n",
            "        [-0.02197415,  0.00516459,  0.03317114, ..., -0.04325645,\n",
            "         -0.02075824, -0.03856363],\n",
            "        [-0.01060683, -0.02133838,  0.01332563, ...,  0.06967086,\n",
            "         -0.04189531,  0.04644488]]], dtype=float32)], [array([[ 0.09414316, -0.00240031, -0.03494994, -0.02601207,  0.11273657,\n",
            "         0.08966121,  0.00034986,  0.17132168, -0.19578227, -0.24868199,\n",
            "         0.19968833,  0.16790016, -0.03285107,  0.18000588, -0.08985255,\n",
            "        -0.19708465,  0.12102418,  0.06202364,  0.1765834 , -0.1337945 ,\n",
            "        -0.0845085 ,  0.17080316,  0.04576069, -0.13604093, -0.10455331,\n",
            "         0.16523325,  0.07084377, -0.16988239, -0.06490609,  0.3015667 ,\n",
            "         0.26721177,  0.13217923,  0.13954593, -0.18323955, -0.21281524,\n",
            "        -0.10914292, -0.22599162,  0.02628164, -0.06100551, -0.27228582,\n",
            "         0.24510579, -0.23793258,  0.11712468, -0.1329286 , -0.15179975,\n",
            "         0.00069306,  0.28001073, -0.13423195,  0.22750808, -0.0857423 ,\n",
            "         0.03390707, -0.13804832,  0.1678915 , -0.24443297, -0.21661364,\n",
            "         0.11187869,  0.09815642, -0.14524597, -0.05189518,  0.12827946,\n",
            "         0.17878935, -0.01422724, -0.02229798,  0.21556076]],\n",
            "      dtype=float32)], [array([[1.1796136 , 1.3109848 , 0.        , 1.134846  , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.6744986 , 0.        ,\n",
            "        0.        , 0.        , 0.8008211 , 0.        , 0.        ,\n",
            "        0.936233  , 1.1056656 , 0.        , 1.4513205 , 0.        ,\n",
            "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.92107713, 0.918604  , 0.        , 1.0182419 , 1.1278054 ,\n",
            "        0.        , 0.84388584]], dtype=float32)], [array([[0.01224363]], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}